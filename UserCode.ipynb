{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "269bc832",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import scipy.stats as sps\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "dataset = pd.read_csv(r'diabetes.csv',header=None)\n",
    "dataset = dataset.sample(frac=1)\n",
    "dataset.columns = ['Pregnancies','Glucose','BloodPressure','SkinThickness','Insulin','BMI','DiabetesPedigreeFunction','Age',\n",
    "           'Outcome']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9356a8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(target_col):\n",
    "    elements,counts = np.unique(target_col,return_counts = True)\n",
    "    entropy = np.sum([(-counts[i]/np.sum(counts))*np.log2(counts[i]/np.sum(counts)) for i in range(len(elements))])\n",
    "    return entropy\n",
    "\n",
    "\n",
    "########################################################################################################### \n",
    "###########################################################################################################\n",
    "\n",
    "\n",
    "def InfoGain(data,split_attribute_name,target_name=\"target\"):\n",
    "    \n",
    "    #Calculate the entropy of the total dataset\n",
    "    total_entropy = entropy(data[target_name])\n",
    "    \n",
    "    ##Calculate the entropy of the dataset\n",
    "    \n",
    "    #Calculate the values and the corresponding counts for the split attribute \n",
    "    vals,counts= np.unique(data[split_attribute_name],return_counts=True)\n",
    "    \n",
    "    #Calculate the weighted entropy\n",
    "    Weighted_Entropy = np.sum([(counts[i]/np.sum(counts))*entropy(data.where(data[split_attribute_name]==vals[i]).dropna()[target_name]) for i in range(len(vals))])\n",
    "    \n",
    "    #Calculate the information gain\n",
    "    Information_Gain = total_entropy - Weighted_Entropy\n",
    "    return Information_Gain\n",
    "       \n",
    "###########################################################################################################\n",
    "###########################################################################################################\n",
    "\n",
    "\n",
    "def ID3(data,originaldata,features,target_attribute_name=\"Outcome\",parent_node_class = None):\n",
    "    #Define the stopping criteria --> If one of this is satisfied, we want to return a leaf node#\n",
    "    \n",
    "    #If all target_values have the same value, return this value\n",
    "    if len(np.unique(data[target_attribute_name])) <= 1:\n",
    "        return np.unique(data[target_attribute_name])[0]\n",
    "    \n",
    "    #If the dataset is empty, return the mode target feature value in the original dataset\n",
    "    elif len(data)==0:\n",
    "        return np.unique(originaldata[target_attribute_name])[np.argmax(np.unique(originaldata[target_attribute_name],return_counts=True)[1])]\n",
    "    \n",
    "    #If the feature space is empty, return the mode target feature value of the direct parent node --> Note that\n",
    "    #the direct parent node is that node which has called the current run of the ID3 algorithm and hence\n",
    "    #the mode target feature value is stored in the parent_node_class variable.\n",
    "    \n",
    "    elif len(features) ==0:\n",
    "        return parent_node_class\n",
    "    \n",
    "    #If none of the above holds true, grow the tree!\n",
    "    \n",
    "    else:\n",
    "        #Set the default value for this node --> The mode target feature value of the current node\n",
    "        parent_node_class = np.unique(data[target_attribute_name])[np.argmax(np.unique(data[target_attribute_name],return_counts=True)[1])]\n",
    "        \n",
    "        \n",
    "        ################################################################################################################\n",
    "        ############!!!!!!!!!Implement the subspace sampling. Draw a number of m = sqrt(p) features!!!!!!!!#############\n",
    "        ###############################################################################################################\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        features = np.random.choice(features,size=np.int_(np.sqrt(len(features))),replace=False)\n",
    "        \n",
    "        #Select the feature which best splits the dataset\n",
    "        item_values = [InfoGain(data,feature,target_attribute_name) for feature in features] #Return the information gain values for the features in the dataset\n",
    "        best_feature_index = np.argmax(item_values)\n",
    "        best_feature = features[best_feature_index]\n",
    "        \n",
    "        #Create the tree structure. The root gets the name of the feature (best_feature) with the maximum information\n",
    "        #gain in the first run\n",
    "        tree = {best_feature:{}}\n",
    "        \n",
    "        #Remove the feature with the best inforamtion gain from the feature space\n",
    "        features = [i for i in features if i != best_feature]\n",
    "        \n",
    "        \n",
    "        #Grow a branch under the root node for each possible value of the root node feature\n",
    "        \n",
    "        for value in np.unique(data[best_feature]):\n",
    "            value = value\n",
    "            #Split the dataset along the value of the feature with the largest information gain and therwith create sub_datasets\n",
    "            sub_data = data.where(data[best_feature] == value).dropna()\n",
    "            \n",
    "            #Call the ID3 algorithm for each of those sub_datasets with the new parameters --> Here the recursion comes in!\n",
    "            subtree = ID3(sub_data,dataset,features,target_attribute_name,parent_node_class)\n",
    "            \n",
    "            #Add the sub tree, grown from the sub_dataset to the tree under the root node\n",
    "            tree[best_feature][value] = subtree\n",
    "            \n",
    "        return(tree)    \n",
    "    \n",
    "                \n",
    "###########################################################################################################\n",
    "###########################################################################################################\n",
    "\n",
    "    \n",
    "def predict(query,tree,default = 'p'):\n",
    "        \n",
    "    for key in list(query.keys()):\n",
    "        if key in list(tree.keys()):\n",
    "            try:\n",
    "                result = tree[key][query[key]] \n",
    "            except:\n",
    "                return default\n",
    "            result = tree[key][query[key]]\n",
    "            if isinstance(result,dict):\n",
    "                return predict(query,result)\n",
    "\n",
    "            else:\n",
    "                return result\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "###########################################################################################################\n",
    "###########################################################################################################\n",
    "\n",
    "def train_test_split(dataset):\n",
    "    training_data = dataset.iloc[:round(0.75*len(dataset))].reset_index(drop=True)#We drop the index respectively relabel the index\n",
    "    #starting form 0, because we do not want to run into errors regarding the row labels / indexes\n",
    "    testing_data = dataset.iloc[round(0.75*len(dataset)):].reset_index(drop=True)\n",
    "    return training_data,testing_data\n",
    "\n",
    "\n",
    "training_data = train_test_split(dataset)[0]\n",
    "testing_data = train_test_split(dataset)[1] \n",
    "\n",
    "\n",
    "\n",
    "###########################################################################################################\n",
    "###########################################################################################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21a2170a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######Train the Random Forest model###########\n",
    "\n",
    "def RandomForest_Train(dataset,number_of_Trees):\n",
    "    #Create a list in which the single forests are stored\n",
    "    random_forest_sub_tree = []\n",
    "    \n",
    "    #Create a number of n models\n",
    "    for i in range(number_of_Trees):\n",
    "        #Create a number of bootstrap sampled datasets from the original dataset \n",
    "        bootstrap_sample = dataset.sample(frac=1,replace=True)\n",
    "        \n",
    "        #Create a training and a testing datset by calling the train_test_split function\n",
    "        bootstrap_training_data = train_test_split(bootstrap_sample)[0]\n",
    "        bootstrap_testing_data = train_test_split(bootstrap_sample)[1] \n",
    "        \n",
    "        \n",
    "        #Grow a tree model for each of the training data\n",
    "        #We implement the subspace sampling in the ID3 algorithm itself. Hence take a look at the ID3 algorithm above!\n",
    "        random_forest_sub_tree.append(ID3(bootstrap_training_data,bootstrap_training_data,bootstrap_training_data.drop(labels=['Outcome'],axis=1).columns))\n",
    "        \n",
    "    return random_forest_sub_tree\n",
    "\n",
    "\n",
    "        \n",
    "random_forest = RandomForest_Train(dataset,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c63b76a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outcome  1\n",
      "prediction:  0\n"
     ]
    }
   ],
   "source": [
    "#######Predict a new query instance###########\n",
    "def RandomForest_Predict(query,random_forest,default='1'):\n",
    "    predictions = []\n",
    "    for tree in random_forest:\n",
    "        predictions.append(predict(query,tree,default))\n",
    "    return sps.mode(predictions)[0][0]\n",
    "\n",
    "\n",
    "query = testing_data.iloc[0,:].drop('Outcome').to_dict()\n",
    "query_target = testing_data.iloc[0,-1]\n",
    "print('Outcome ',query_target)\n",
    "prediction = RandomForest_Predict(query,random_forest)\n",
    "print('prediction: ',prediction)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b32ffc3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      "[[115   4]\n",
      " [ 37  36]]\n",
      "\n",
      "Whole Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.97      0.85       119\n",
      "           1       0.90      0.49      0.64        73\n",
      "\n",
      "    accuracy                           0.79       192\n",
      "   macro avg       0.83      0.73      0.74       192\n",
      "weighted avg       0.81      0.79      0.77       192\n",
      "\n",
      "################################################################################\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "78.64583333333334"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#######Test the model on the testing data and return the accuracy###########\n",
    "def RandomForest_Test(data,random_forest):\n",
    "    data['predictions'] = None\n",
    "    for i in range(len(data)):\n",
    "        query = data.iloc[i,:].drop('Outcome').to_dict()\n",
    "        data.loc[i,'predictions'] = RandomForest_Predict(query,random_forest,default='1')\n",
    "#     print(\"Predictions for correct\",sum(data['predictions']=='1'))\n",
    "#     print(\"Prections for wrong\",sum(data['predictions']=='0'))\n",
    "#     print(\"Actual Outcome for correct\",sum(data['Outcome']=='1'))\n",
    "#     print(\"Actual Outcome for wrong\",sum(data['Outcome']=='0'))\n",
    "    accuracy = sum(data['predictions'] == data['Outcome'])/len(data)*100\n",
    "#     print('The prediction accuracy is: ',sum(data['predictions'] == data['target'])/len(data)*100,'%')\n",
    "    \n",
    "    print(\"Confusion Matrix: \")\n",
    "    print(metrics.confusion_matrix(data['Outcome'],data['predictions'], labels=[\"0\",\"1\"]))\n",
    "    print()\n",
    "    \n",
    "    print(\"Whole Classification Report:\")\n",
    "    print(metrics.classification_report(data['Outcome'],data['predictions'], labels=[\"0\",\"1\"]))\n",
    "    print(\"################################################################################\")\n",
    "    \n",
    "    \n",
    "    return accuracy\n",
    "        \n",
    "        \n",
    "        \n",
    "RandomForest_Test(testing_data,random_forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f31b31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257b8c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of Trees:  1\n",
      "Confusion Matrix: \n",
      "[[82 20]\n",
      " [ 8 48]]\n",
      "\n",
      "Whole Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.69      0.78       119\n",
      "           1       0.71      0.66      0.68        73\n",
      "\n",
      "   micro avg       0.82      0.68      0.74       192\n",
      "   macro avg       0.81      0.67      0.73       192\n",
      "weighted avg       0.83      0.68      0.75       192\n",
      "\n",
      "################################################################################\n",
      "No. of Trees:  2\n",
      "Confusion Matrix: \n",
      "[[116   3]\n",
      " [ 51  22]]\n",
      "\n",
      "Whole Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.97      0.81       119\n",
      "           1       0.88      0.30      0.45        73\n",
      "\n",
      "    accuracy                           0.72       192\n",
      "   macro avg       0.79      0.64      0.63       192\n",
      "weighted avg       0.77      0.72      0.67       192\n",
      "\n",
      "################################################################################\n",
      "No. of Trees:  3\n",
      "Confusion Matrix: \n",
      "[[109   3]\n",
      " [ 17  50]]\n",
      "\n",
      "Whole Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.92      0.89       119\n",
      "           1       0.94      0.68      0.79        73\n",
      "\n",
      "   micro avg       0.89      0.83      0.86       192\n",
      "   macro avg       0.90      0.80      0.84       192\n",
      "weighted avg       0.89      0.83      0.85       192\n",
      "\n",
      "################################################################################\n",
      "No. of Trees:  4\n",
      "Confusion Matrix: \n",
      "[[112   4]\n",
      " [ 12  56]]\n",
      "\n",
      "Whole Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.94      0.92       119\n",
      "           1       0.93      0.77      0.84        73\n",
      "\n",
      "   micro avg       0.91      0.88      0.89       192\n",
      "   macro avg       0.92      0.85      0.88       192\n",
      "weighted avg       0.91      0.88      0.89       192\n",
      "\n",
      "################################################################################\n",
      "No. of Trees:  5\n",
      "Confusion Matrix: \n",
      "[[112   4]\n",
      " [  6  65]]\n",
      "\n",
      "Whole Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.94      0.95       119\n",
      "           1       0.94      0.89      0.92        73\n",
      "\n",
      "   micro avg       0.95      0.92      0.93       192\n",
      "   macro avg       0.95      0.92      0.93       192\n",
      "weighted avg       0.95      0.92      0.93       192\n",
      "\n",
      "################################################################################\n",
      "No. of Trees:  6\n",
      "Confusion Matrix: \n",
      "[[114   3]\n",
      " [ 23  47]]\n",
      "\n",
      "Whole Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.96      0.89       119\n",
      "           1       0.94      0.64      0.76        73\n",
      "\n",
      "   micro avg       0.86      0.84      0.85       192\n",
      "   macro avg       0.89      0.80      0.83       192\n",
      "weighted avg       0.87      0.84      0.84       192\n",
      "\n",
      "################################################################################\n",
      "No. of Trees:  7\n",
      "Confusion Matrix: \n",
      "[[115   1]\n",
      " [ 17  56]]\n",
      "\n",
      "Whole Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.97      0.92       119\n",
      "           1       0.98      0.77      0.86        73\n",
      "\n",
      "   micro avg       0.90      0.89      0.90       192\n",
      "   macro avg       0.93      0.87      0.89       192\n",
      "weighted avg       0.91      0.89      0.90       192\n",
      "\n",
      "################################################################################\n"
     ]
    }
   ],
   "source": [
    "##############################################################################################################\n",
    "##########Plot the prediction accuracy with respect to the number of Trees in the random forests#############\n",
    "##############################################################################################################\n",
    "\n",
    "\n",
    "accuracy = []\n",
    "\n",
    "for i in range(1,11,1):\n",
    "    random_forest = RandomForest_Train(dataset,i)\n",
    "    print(\"No. of Trees: \",i)\n",
    "    accuracy.append(RandomForest_Test(testing_data,random_forest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f1823e",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695ddd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,10))\n",
    "\n",
    "ax0 = fig.add_subplot(111)\n",
    "\n",
    "ax0.plot(np.logspace(0,1,10),accuracy)\n",
    "ax0.set_yticks(np.linspace(50,100,50))\n",
    "ax0.set_title(\"Proposed Accuracy Random Forest Graph\")\n",
    "ax0.set_xscale('log')\n",
    "ax0.set_xlabel(\"Trees\")\n",
    "ax0.set_ylabel('Sensitivity(%)')\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
